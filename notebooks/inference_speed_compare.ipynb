{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import all the dependency\n",
    "%cd ..\n",
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "from medusa.model.medusa_model import MedusaModel\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import numpy as np\n",
    "from medusa.model.utils import *\n",
    "from medusa.model.kv_cache import *\n",
    "from medusa.model.medusa_choices import mc_sim_7b_63\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# medusa inference forward time test function\n",
    "\n",
    "@contextmanager\n",
    "def timed(wall_times, key):\n",
    "    start = time.time()\n",
    "    torch.cuda.synchronize()\n",
    "    yield\n",
    "    torch.cuda.synchronize()\n",
    "    end = time.time()\n",
    "    elapsed_time = end - start\n",
    "    wall_times[key].append(elapsed_time)\n",
    "\n",
    "def medusa_forward(input_ids, model, tokenizer, medusa_choices, temperature, posterior_threshold, posterior_alpha, max_steps = 512):\n",
    "    wall_times = {'medusa': [], 'tree': [], 'posterior': [], 'update': [], 'init': []}\n",
    "\n",
    "    with timed(wall_times, 'init'):\n",
    "        if hasattr(model, \"medusa_choices\") and model.medusa_choices == medusa_choices:\n",
    "            # Load the cached medusa buffer\n",
    "            medusa_buffers = model.medusa_buffers\n",
    "        else:\n",
    "            # Initialize the medusa buffer\n",
    "            medusa_buffers = generate_medusa_buffers(\n",
    "                medusa_choices, device=model.base_model.device\n",
    "            )\n",
    "        model.medusa_buffers = medusa_buffers\n",
    "        model.medusa_choices = medusa_choices\n",
    "\n",
    "        # Initialize the past key and value states\n",
    "        if hasattr(model, \"past_key_values\"):\n",
    "            past_key_values = model.past_key_values\n",
    "            past_key_values_data = model.past_key_values_data\n",
    "            current_length_data = model.current_length_data\n",
    "            # Reset the past key and value states\n",
    "            current_length_data.zero_()\n",
    "        else:\n",
    "            (\n",
    "                past_key_values,\n",
    "                past_key_values_data,\n",
    "                current_length_data,\n",
    "            ) = initialize_past_key_values(model.base_model)\n",
    "            model.past_key_values = past_key_values\n",
    "            model.past_key_values_data = past_key_values_data\n",
    "            model.current_length_data = current_length_data\n",
    "\n",
    "        input_len = input_ids.shape[1]\n",
    "        reset_medusa_mode(model)\n",
    "        medusa_logits, logits = initialize_medusa(\n",
    "                input_ids, model, medusa_buffers[\"medusa_attn_mask\"], past_key_values\n",
    "        )\n",
    "    new_token = 0\n",
    "\n",
    "    for idx in range(max_steps):\n",
    "        with timed(wall_times, 'medusa'):\n",
    "            candidates, tree_candidates = generate_candidates(\n",
    "                    medusa_logits,\n",
    "                    logits,\n",
    "                    medusa_buffers[\"tree_indices\"],\n",
    "                    medusa_buffers[\"retrieve_indices\"],\n",
    "                )\n",
    "\n",
    "        with timed(wall_times, 'tree'):\n",
    "            medusa_logits, logits, outputs = tree_decoding(\n",
    "                    model,\n",
    "                    tree_candidates,\n",
    "                    past_key_values,\n",
    "                    medusa_buffers[\"medusa_position_ids\"],\n",
    "                    input_ids,\n",
    "                    medusa_buffers[\"retrieve_indices\"],\n",
    "                )\n",
    "\n",
    "        with timed(wall_times, 'posterior'):\n",
    "            best_candidate, accept_length = evaluate_posterior(\n",
    "                    logits, candidates, temperature, posterior_threshold, posterior_alpha\n",
    "                )\n",
    "\n",
    "        with timed(wall_times, 'update'):\n",
    "            input_ids, logits, medusa_logits, new_token = update_inference_inputs(\n",
    "                    input_ids,\n",
    "                    candidates,\n",
    "                    best_candidate,\n",
    "                    accept_length,\n",
    "                    medusa_buffers[\"retrieve_indices\"],\n",
    "                    outputs,\n",
    "                    logits,\n",
    "                    medusa_logits,\n",
    "                    new_token,\n",
    "                    past_key_values_data,\n",
    "                    current_length_data,\n",
    "                )\n",
    "\n",
    "        if tokenizer.eos_token_id in input_ids[0, input_len:].tolist():\n",
    "            break\n",
    "\n",
    "    return input_ids, new_token, idx, wall_times"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load medusa model and set hyper-params\n",
    "base_model = \"/alg_vepfs/public/hqy/baichuan/wizardlm-10-16-dsl/checkpoint-100\"\n",
    "medusa_path = \"/root/zhaliangyu/llm_inference_acc/Medusa/medusa_weights/wizardlm13b_medusa\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "model = MedusaModel.from_pretrained(\n",
    "    base_model=base_model,\n",
    "    medusa_head_name_or_path=medusa_path,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "tokenizer = model.get_tokenizer()\n",
    "temperature = 0.\n",
    "posterior_threshold = 0.09\n",
    "posterior_alpha = 0.3"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "PROMPT = \"数据库的信息如下所示：{db_info},\\n schema'和'detail'表示数据库的内容; 'foreign_keys'表示数据库多表之间的连接关系. \" \\\n",
    "         \"根据数据库信息以及用户的输入生成符合json格式输出的指令. \\n\\nUSER: {user_query}\\nASSISTANT:\"\n",
    "\n",
    "db_info = \"\"\"{\"db_name\": \"Twenty-Five_Cents\", \"db_info\": {\"Twenty-Five_Cents\": {\"numeric_info\": {\"Year\": [2006, 2007, 2007], \"Issue price\": [24.95, 24.95, 24.95]}, \"categorical_info\": {\"Theme\": [\"Calgary Flames\", \"Edmonton Oilers\", \"Montreal Canadiens\", \"Ottawa Senators\", \"Toronto Maple Leafs\", \"Vancouver Canucks\"], \"Artist\": [\"N/A\"], \"Mintage\": [\"1264\", \"1634\", \"2213\", \"2952\", \"3527\", \"832\", \"N/A\"]}, \"date_cols_info\": {}}}, \"foreign_keys\": []}\"\"\"\n",
    "db_info = db_info.replace(\"{\",\"{{\").replace(\"}\", \"}}\")\n",
    "user_query = \"不同主题的平均发行价格是多少？\"\n",
    "PROMPT = PROMPT.format(db_info=db_info,\n",
    "                       user_query=user_query)\n",
    "medusa_choices = mc_sim_7b_63\n",
    "\n",
    "with torch.inference_mode():\n",
    "    input_ids = tokenizer([PROMPT]).input_ids\n",
    "    output_ids, new_token, idx, wall_time = medusa_forward(\n",
    "                    torch.as_tensor(input_ids).cuda(),\n",
    "                    model,\n",
    "                    tokenizer,\n",
    "                    medusa_choices,\n",
    "                    temperature,\n",
    "                    posterior_threshold,\n",
    "                    posterior_alpha,\n",
    "                )\n",
    "    print(\"Output length:\", output_ids.size(-1))\n",
    "    print(\"Compression ratio:\", new_token / idx)\n",
    "\n",
    "output = tokenizer.decode(\n",
    "                    output_ids,\n",
    "                    spaces_between_special_tokens=False,\n",
    "                )\n",
    "print(output)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "max_length = 50\n",
    "\n",
    "def format_string(text, value, max_length):\n",
    "    value_str = \"{:.3f}\".format(value)\n",
    "    return f\"{text:<{max_length - len(value_str)}}{value_str}\"\n",
    "\n",
    "time_init = np.sum(wall_time['init'] )\n",
    "time_medusa = np.sum(wall_time['medusa'] )\n",
    "time_tree = np.sum(wall_time['tree'] )\n",
    "time_posterior = np.sum(wall_time['posterior'] )\n",
    "time_update = np.sum(wall_time['update'] )\n",
    "time_total = time_init + time_medusa + time_tree + time_posterior + time_update\n",
    "\n",
    "print('='*max_length)\n",
    "print(format_string(\"Wall time init: \", time_init, max_length))\n",
    "print(format_string(\"Wall time medusa: \", time_medusa, max_length))\n",
    "print(format_string(\"Wall time Tree: \", time_tree, max_length))\n",
    "print(format_string(\"Wall time Posterior: \", time_posterior, max_length))\n",
    "print(format_string(\"Wall time Update: \", time_update, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Wall time portion medusa: \", time_medusa / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Tree: \", time_tree / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Posterior: \", time_posterior / time_total, max_length))\n",
    "print(format_string(\"Wall time portion Update: \", time_update / time_total, max_length))\n",
    "print('-'*max_length)\n",
    "print(format_string(\"Tokens/second: \", new_token / time_total, max_length))\n",
    "print('='*max_length)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cache_dir = \"./\"\n",
    "model_max_length = 2048\n",
    "device = \"cuda\"\n",
    "\n",
    "config = transformers.AutoConfig.from_pretrained(\n",
    "    base_model,\n",
    "    cache_dir=cache_dir,\n",
    ")\n",
    "\n",
    "original_model = transformers.AutoModelForMaskedLM.from_pretrained(\n",
    "    base_model,\n",
    "    config=config,\n",
    "    cache_dir=cache_dir,\n",
    "    low_cpu_mem_usage=False,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "original_model.eval()\n",
    "\n",
    "ori_tokenizer = transformers.AutoTokenizer.from_pretrained(\n",
    "    base_model,\n",
    "    config=config,\n",
    "    model_max_length=model_max_length,\n",
    "    padding_side=\"right\",\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "generate_configs = transformers.GenerationConfig(\n",
    "    temperature=0.1,\n",
    "    top_p=0.9,\n",
    "    top_k=10\n",
    ")\n",
    "\n",
    "input_ids = ori_tokenizer([PROMPT]).input_ids.to(device)\n",
    "start = time.time()\n",
    "with torch.no_grad():\n",
    "    output_ids = original_model.generate(\n",
    "        input_ids=input_ids,\n",
    "        generation_config=generate_configs,\n",
    "        return_dict_in_generate=True,\n",
    "        output_scores=True,\n",
    "        max_new_tokens=512,\n",
    "    )\n",
    "s = output_ids.sequences\n",
    "new_token_num = len(s[0])\n",
    "output = tokenizer.batch_decode(s, skip_special_tokens=True)\n",
    "output = output[0].split(\"ASSISTANT:\")[1].strip()\n",
    "end = time.time()\n",
    "original_model_total = end - start\n",
    "print(\"Original generate total time:\", original_model_total)\n",
    "print(\"Original Tokens/second: \", original_model_total / new_token_num)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}